





# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

# Set Seaborn style
sns.set(style="whitegrid")

# Load the Telco Churn dataset
df = pd.read_csv('../data/telco_churn.csv')

# Preview first 5 rows
df.head()


# Check dataset shape
print(f"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.")

# View column names and data types
df.info()


# Check for missing values
print("Missing values in each column:")
print(df.isnull().sum())

# Display number of unique values in each column
print("\nUnique values per column:")
print(df.nunique())





# Convert TotalCharges to numeric (coerce invalid entries to NaN)
df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')

# Check how many values became NaN after conversion
print("Missing values in TotalCharges after conversion:", df['TotalCharges'].isna().sum())

# Optional: View affected rows
df[df['TotalCharges'].isna()]





# Drop rows with missing TotalCharges
df = df[df['TotalCharges'].notna()]

# Reset index after dropping
df.reset_index(drop=True, inplace=True)

# Confirm new shape
print(f"New dataset shape: {df.shape}")





# Check distribution of target variable (Churn)
print("Target Variable Distribution:")
print(df['Churn'].value_counts())

# Plot churn distribution
sns.countplot(x='Churn', data=df, palette='Set2')
plt.title("Churn Distribution")
plt.xlabel("Churn")
plt.ylabel("Number of Customers")
plt.show()





# Drop customerID (not useful for ML)
df.drop('customerID', axis=1, inplace=True)

# Separate categorical and numerical features
categorical_cols = df.select_dtypes(include='object').columns.tolist()
numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()

# Print them
print("Categorical columns:")
print(categorical_cols)

print("\nNumerical columns:")
print(numerical_cols)





# Strip whitespace from all string (object) columns
df[categorical_cols] = df[categorical_cols].apply(lambda x: x.str.strip())

# Confirm unique values in a few key columns
for col in ['MultipleLines', 'InternetService', 'OnlineSecurity', 'Churn']:
    print(f"\nUnique values in {col}: {df[col].unique()}")





# Encode target variable
df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})

# Verify encoding
print(df['Churn'].value_counts())





# Plot distributions of numerical features
num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']

plt.figure(figsize=(15, 4))
for i, col in enumerate(num_cols):
    plt.subplot(1, 3, i+1)
    sns.histplot(df[col], kde=True, bins=30, color='skyblue')
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
plt.tight_layout()
plt.show()








# Plot distribution of Contract type
plt.figure(figsize=(6,4))
sns.countplot(x='Contract', data=df, palette='pastel')
plt.title("Contract Type Distribution")
plt.xlabel("Contract Type")
plt.ylabel("Number of Customers")
plt.show()

# Calculate churn rate by contract type
contract_churn = df.groupby('Contract')['Churn'].mean().round(2) * 100
print("Churn Rate by Contract Type (%):\n", contract_churn)








# Plot distribution of Internet Service type
plt.figure(figsize=(6,4))
sns.countplot(x='InternetService', data=df, palette='pastel')
plt.title("Internet Service Distribution")
plt.xlabel("Internet Service")
plt.ylabel("Number of Customers")
plt.show()

# Calculate churn rate by InternetService
internet_churn = df.groupby('InternetService')['Churn'].mean().round(2) * 100
print("Churn Rate by Internet Service (%):\n", internet_churn)








# Plot distribution of OnlineSecurity service
plt.figure(figsize=(6,4))
sns.countplot(x='OnlineSecurity', data=df, palette='pastel')
plt.title("Online Security Service Distribution")
plt.xlabel("Online Security")
plt.ylabel("Number of Customers")
plt.show()

# Calculate churn rate by OnlineSecurity
security_churn = df.groupby('OnlineSecurity')['Churn'].mean().round(2) * 100
print("Churn Rate by Online Security (%):\n", security_churn)








# Plot distribution of Payment Method
plt.figure(figsize=(8,3))
sns.countplot(x='PaymentMethod', data=df, palette='pastel')
plt.title("Payment Method Distribution")
plt.xlabel("Payment Method")
plt.ylabel("Number of Customers")
plt.xticks(rotation=45)
plt.show()

# Calculate churn rate by Payment Method
payment_churn = df.groupby('PaymentMethod')['Churn'].mean().round(2) * 100
print("Churn Rate by Payment Method (%):\n", payment_churn)








# Plot distribution of Tech Support service
plt.figure(figsize=(6,4))
sns.countplot(x='TechSupport', data=df, palette='pastel')
plt.title("Tech Support Service Distribution")
plt.xlabel("Tech Support")
plt.ylabel("Number of Customers")
plt.show()

# Calculate churn rate by Tech Support
tech_churn = df.groupby('TechSupport')['Churn'].mean().round(2) * 100
print("Churn Rate by Tech Support (%):\n", tech_churn)








# Plot distribution of Paperless Billing
plt.figure(figsize=(6,4))
sns.countplot(x='PaperlessBilling', data=df, palette='pastel')
plt.title("Paperless Billing Distribution")
plt.xlabel("Paperless Billing")
plt.ylabel("Number of Customers")
plt.show()

# Calculate churn rate by Paperless Billing
billing_churn = df.groupby('PaperlessBilling')['Churn'].mean().round(2) * 100
print("Churn Rate by Paperless Billing (%):\n", billing_churn)








# Features: gender, SeniorCitizen, Partner, Dependents
cols = ['gender', 'SeniorCitizen', 'Partner', 'Dependents']

plt.figure(figsize=(8, 6))

for i, col in enumerate(cols, 1):
    plt.subplot(2, 2, i)
    sns.countplot(x=col, data=df, palette='pastel')
    plt.title(f"{col} Distribution")
    plt.xlabel(col)
    plt.ylabel("Number of Customers")

plt.tight_layout()
plt.show()





# Features: PhoneService, MultipleLines, StreamingTV, StreamingMovies
cols = ['PhoneService', 'MultipleLines', 'StreamingTV', 'StreamingMovies']

plt.figure(figsize=(8, 6))  
for i, col in enumerate(cols, 1):
    plt.subplot(2, 2, i)
    sns.countplot(x=col, data=df, palette='pastel')
    plt.title(f"{col} Distribution")
    plt.xlabel(col)
    plt.ylabel("Count")
    plt.xticks(rotation=15)  

plt.tight_layout(pad=2.0)
plt.show()








from sklearn.model_selection import train_test_split

# Separate features and target
X = df.drop('Churn', axis=1)
y = df['Churn']

# Train-test split (80/20) with stratification
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Sanity check
print("Shapes:")
print("X_train:", X_train.shape, "X_test:", X_test.shape)
print("y_train:", y_train.shape, "y_test:", y_test.shape)

print("\nChurn ratio:")
print("Overall:", y.mean().round(3))
print("Train: ", y_train.mean().round(3))
print("Test:  ", y_test.mean().round(3))





from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Get feature types again (from training set only)
categorical_cols = X_train.select_dtypes(include='object').columns.tolist()
numerical_cols = X_train.select_dtypes(include=['int64','float64']).columns.tolist()

print("Categorical:", categorical_cols)
print("Numerical:", numerical_cols)

# Preprocessor: scale numeric + one-hot encode categorical
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numerical_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore", drop="if_binary"), categorical_cols),
    ]
)

# Logistic Regression (baseline)
log_reg = LogisticRegression(class_weight="balanced", solver="liblinear", max_iter=1000)

# Full pipeline
pipe_lr = Pipeline(steps=[("prep", preprocessor),
                         ("model", log_reg)])

# Fit pipeline
pipe_lr.fit(X_train, y_train)

# Check feature count after preprocessing
n_feats = pipe_lr.named_steps["prep"].transform(X_train).shape[1]
print("Features after preprocessing:", n_feats)





from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report,
    precision_recall_curve
)
import matplotlib.pyplot as plt
import seaborn as sns

# Predictions
y_pred = pipe_lr.predict(X_test)
y_proba = pipe_lr.predict_proba(X_test)[:, 1]

# Core metrics
acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec  = recall_score(y_test, y_pred)
f1   = f1_score(y_test, y_pred)
auc  = roc_auc_score(y_test, y_proba)

print("=== Baseline Logistic Regression (Pipeline) ===")
print(f"Accuracy:  {acc:.3f}")
print(f"Precision: {prec:.3f}")
print(f"Recall:    {rec:.3f}")
print(f"F1-score:  {f1:.3f}")
print(f"ROC-AUC:   {auc:.3f}\n")

print("Classification Report:\n", classification_report(y_test, y_pred, digits=3))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(4,3))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,
            xticklabels=['Pred 0','Pred 1'], yticklabels=['True 0','True 1'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(figsize=(4,3))
plt.plot(fpr, tpr, label=f"ROC AUC = {auc:.3f}")
plt.plot([0,1], [0,1], linestyle='--')
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.tight_layout()
plt.show()

# Precision-Recall Curve
prec_curve, rec_curve, _ = precision_recall_curve(y_test, y_proba)
plt.figure(figsize=(4,3))
plt.plot(rec_curve, prec_curve)
plt.title("Precision-Recall Curve")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.tight_layout()
plt.show()





from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.pipeline import Pipeline

# Random Forest pipeline (reuse preprocessor)
rf_clf = Pipeline(steps=[
    ("prep", preprocessor),
    ("model", RandomForestClassifier(
        n_estimators=300,       # a bit higher for stability
        max_depth=None,
        class_weight="balanced",
        n_jobs=-1,              # <-- use all cores
        random_state=42
    ))
])

# Fit
rf_clf.fit(X_train, y_train)

# Predict
y_pred_rf = rf_clf.predict(X_test)
y_proba_rf = rf_clf.predict_proba(X_test)[:, 1]

# Metrics
acc  = accuracy_score(y_test, y_pred_rf)
prec = precision_score(y_test, y_pred_rf)
rec  = recall_score(y_test, y_pred_rf)
f1   = f1_score(y_test, y_pred_rf)
auc  = roc_auc_score(y_test, y_proba_rf)

print("=== Random Forest Classifier ===")
print(f"Accuracy:  {acc:.3f}")
print(f"Precision: {prec:.3f}")
print(f"Recall:    {rec:.3f}")
print(f"F1-score:  {f1:.3f}")
print(f"ROC-AUC:   {auc:.3f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred_rf, digits=3))





from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report
from sklearn.pipeline import Pipeline
import numpy as np

# Compute scale_pos_weight from training data: (negatives / positives)
pos = np.sum(y_train == 1)
neg = np.sum(y_train == 0)
spw = neg / pos
print(f"scale_pos_weight (neg/pos): {spw:.2f}")

# XGBoost pipeline
xgb_clf = Pipeline(steps=[
    ("prep", preprocessor),
    ("model", XGBClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=5,
        subsample=0.8,
        colsample_bytree=0.8,
        scale_pos_weight=spw,     # <- dynamic imbalance handling
        random_state=42,
        n_jobs=-1,
        use_label_encoder=False,
        eval_metric='logloss'
    ))
])

# Fit
xgb_clf.fit(X_train, y_train)

# Predict
y_pred_xgb = xgb_clf.predict(X_test)
y_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]

# Evaluate
acc  = accuracy_score(y_test, y_pred_xgb)
prec = precision_score(y_test, y_pred_xgb)
rec  = recall_score(y_test, y_pred_xgb)
f1   = f1_score(y_test, y_pred_xgb)
auc  = roc_auc_score(y_test, y_proba_xgb)

print("=== XGBoost Classifier ===")
print(f"Accuracy:  {acc:.3f}")
print(f"Precision: {prec:.3f}")
print(f"Recall:    {rec:.3f}")
print(f"F1-score:  {f1:.3f}")
print(f"ROC-AUC:   {auc:.3f}\n")
print("Classification Report:\n", classification_report(y_test, y_pred_xgb, digits=3))





import matplotlib.pyplot as plt
import pandas as pd

# Extract feature names after preprocessing
feature_names = (
    xgb_clf.named_steps["prep"]
    .transformers_[1][1]
    .get_feature_names_out(xgb_clf.named_steps["prep"].transformers_[1][2])
    .tolist()
)
feature_names = xgb_clf.named_steps["prep"].transformers_[0][2] + feature_names

# Get feature importances
importances = xgb_clf.named_steps["model"].feature_importances_

# Create DataFrame for easy sorting
feat_imp_df = pd.DataFrame({
    "Feature": feature_names,
    "Importance": importances
}).sort_values(by="Importance", ascending=False)

# Plot top 10 features
plt.figure(figsize=(8,5))
sns.barplot(x="Importance", y="Feature", data=feat_imp_df.head(10), palette="viridis")
plt.title("Top 10 Important Features Influencing Churn (XGBoost)")
plt.xlabel("Feature Importance Score")
plt.ylabel("Feature")
plt.show()

# Display top 10 features
feat_imp_df.head(10)





!pip install shap -q


import numpy as np
import pandas as pd
import shap

# 1) Pull trained pieces
xgb_model = xgb_clf.named_steps["model"]
prep      = xgb_clf.named_steps["prep"]

# 2) Transform TRAIN and TEST exactly as model sees them
X_train_tx = prep.transform(X_train)
X_test_tx  = prep.transform(X_test)

# Densify if sparse (needed for some SHAP/plot backends)
if hasattr(X_train_tx, "toarray"):
    X_train_tx = X_train_tx.toarray()
if hasattr(X_test_tx, "toarray"):
    X_test_tx = X_test_tx.toarray()

# 3) Feature names: numeric first, then expanded OHE categorical
ohe = prep.named_transformers_["cat"]
cat_feature_names = ohe.get_feature_names_out(categorical_cols)
feature_names = np.concatenate([numerical_cols, cat_feature_names])

print("Train transformed shape:", X_train_tx.shape)
print("Test transformed shape :", X_test_tx.shape)
print("Feature names count    :", len(feature_names))

# 4) Build TreeExplainer with a small TRAIN background sample (stable & fast)
#    Use ~1000 rows or all rows if small.
bg_idx = np.random.RandomState(42).choice(X_train_tx.shape[0], 
                                          size=min(1000, X_train_tx.shape[0]), 
                                          replace=False)
background = X_train_tx[bg_idx]

explainer = shap.TreeExplainer(xgb_model, data=background)
shap_values = explainer.shap_values(X_test_tx)  # shape: (n_samples, n_features) for binary

# 5) GLOBAL IMPORTANCE (bar)
shap.summary_plot(shap_values, features=X_test_tx, feature_names=feature_names, plot_type="bar")

# 6) GLOBAL DISTRIBUTION (beeswarm)
shap.summary_plot(shap_values, features=X_test_tx, feature_names=feature_names)

# 7) LOCAL EXPLANATION for a churned example if available
#    Find first churned example in test set; else use first row
try:
    churn_indices = np.where(y_test.values == 1)[0]
    idx = int(churn_indices[0]) if len(churn_indices) > 0 else 0
except Exception:
    idx = 0

# Handle expected_value shape across SHAP versions
exp_val = explainer.expected_value
if isinstance(exp_val, (list, np.ndarray)) and np.ndim(exp_val) > 0:
    exp_val = exp_val[0]  # binary case

print("Explaining test row index:", idx)

# Force plot (static)
shap.force_plot(
    base_value=exp_val,
    shap_values=shap_values[idx, :],
    features=X_test_tx[idx, :],
    feature_names=feature_names,
    matplotlib=True
)

# Waterfall (legacy fallback, often renders nicer in notebooks)
try:
    from shap.plots import _waterfall
    _waterfall.waterfall_legacy(
        exp_val,
        shap_values[idx],
        feature_names=feature_names,
        max_display=12
    )
except Exception as e:
    print("Waterfall plot fallback skipped:", e)

# 8) Top-10 SHAP features table (mean |SHAP|)
shap_importance = np.abs(shap_values).mean(axis=0)
shap_summary = pd.DataFrame({"Feature": feature_names,
                             "Mean |SHAP|": shap_importance}) \
                  .sort_values("Mean |SHAP|", ascending=False) \
                  .head(10)
shap_summary





import pandas as pd
import matplotlib.pyplot as plt

# Collect metrics you already computed
# (Fill these from your actual runs if they differ)
metrics = [
    {
        "Model": "Logistic Regression",
        "Accuracy": 0.725,   # replace with your LR value
        "Precision": 0.489,
        "Recall": 0.794,
        "F1": 0.606,
        "ROC_AUC": 0.835
    },
    {
        "Model": "Random Forest",
        "Accuracy": 0.785,   # from your RF output
        "Precision": 0.624,
        "Recall": 0.479,
        "F1": 0.542,
        "ROC_AUC": 0.816
    },
    {
        "Model": "XGBoost",
        "Accuracy": 0.746,   # from your XGB output
        "Precision": 0.515,
        "Recall": 0.749,
        "F1": 0.610,
        "ROC_AUC": 0.827
    }
]

df_cmp = pd.DataFrame(metrics)

# Show table
display(df_cmp.style.format({c: "{:.3f}" for c in df_cmp.columns if c != "Model"}))

# Bar chart for quick visual comparison (use F1 and ROC-AUC commonly)
plt.figure(figsize=(8,4))
for i, metric in enumerate(["F1", "ROC_AUC"], start=1):
    plt.subplot(1, 2, i)
    plt.bar(df_cmp["Model"], df_cmp[metric])
    plt.title(metric + " by Model")
    plt.xticks(rotation=15)
    plt.ylabel(metric)
plt.tight_layout()
plt.show()





import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import (
    precision_recall_curve, confusion_matrix, precision_score, recall_score, f1_score
)
import seaborn as sns

# 1) Get predicted probabilities from your fitted XGBoost pipeline
y_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]

# 2) Precision-Recall across thresholds
precisions, recalls, thresholds = precision_recall_curve(y_test, y_proba_xgb)

# Avoid NaNs at the first point (thresholds has len-1 of precisions/recalls)
thr = np.r_[0, thresholds]  # align sizes for plotting

# 3) Choose thresholds by different criteria
# a) Maximize F1
f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)
best_idx = np.nanargmax(f1s)
best_thr_f1 = thr[best_idx]
best_f1 = f1s[best_idx]

# b) Target high recall (e.g., >= 0.80). Pick smallest threshold achieving it.
target_recall = 0.80
idx_recall = np.where(recalls >= target_recall)[0]
best_thr_recall = thr[idx_recall[-1]] if len(idx_recall) > 0 else 0.5  # conservative fallback

print(f"Best F1 threshold: {best_thr_f1:.3f}  (F1={best_f1:.3f})")
print(f"Threshold for Recall ≥ {target_recall:.2f}: {best_thr_recall:.3f}")

# 4) Evaluate XGBoost at each chosen threshold
def eval_at_threshold(t):
    y_pred_t = (y_proba_xgb >= t).astype(int)
    p = precision_score(y_test, y_pred_t)
    r = recall_score(y_test, y_pred_t)
    f = f1_score(y_test, y_pred_t)
    cm = confusion_matrix(y_test, y_pred_t)
    return p, r, f, cm

p_f1, r_f1, f_f1, cm_f1 = eval_at_threshold(best_thr_f1)
p_rec, r_rec, f_rec, cm_rec = eval_at_threshold(best_thr_recall)

print("\n=== Metrics @ Best-F1 Threshold ===")
print(f"Threshold={best_thr_f1:.3f} | Precision={p_f1:.3f} | Recall={r_f1:.3f} | F1={f_f1:.3f}")
print("Confusion Matrix:\n", cm_f1)

print("\n=== Metrics @ Recall-Target Threshold ===")
print(f"Threshold={best_thr_recall:.3f} | Precision={p_rec:.3f} | Recall={r_rec:.3f} | F1={f_rec:.3f}")
print("Confusion Matrix:\n", cm_rec)

# 5) Plot Precision/Recall vs Threshold
plt.figure(figsize=(8,4))
plt.plot(thr, precisions, label="Precision")
plt.plot(thr, recalls, label="Recall")
plt.axvline(best_thr_f1, linestyle="--", label=f"Best F1 thr={best_thr_f1:.2f}")
plt.axvline(best_thr_recall, linestyle=":", label=f"Recall≥{target_recall:.0%} thr={best_thr_recall:.2f}")
plt.xlabel("Threshold")
plt.ylabel("Score")
plt.title("Precision & Recall vs Threshold (XGBoost)")
plt.legend()
plt.tight_layout()
plt.show()

# 6) Visualize confusion matrix at Best-F1 threshold
plt.figure(figsize=(4,3))
sns.heatmap(cm_f1, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Pred 0','Pred 1'], yticklabels=['True 0','True 1'])
plt.title(f"Confusion Matrix @ Best-F1 thr={best_thr_f1:.2f}")
plt.xlabel("Predicted"); plt.ylabel("Actual"); plt.tight_layout()
plt.show()

# 7) Visualize confusion matrix at Recall-target threshold
plt.figure(figsize=(4,3))
sns.heatmap(cm_rec, annot=True, fmt="d", cmap="Blues",
            xticklabels=['Pred 0','Pred 1'], yticklabels=['True 0','True 1'])
plt.title(f"Confusion Matrix @ Recall≥{target_recall:.0%} thr={best_thr_recall:.2f}")
plt.xlabel("Predicted"); plt.ylabel("Actual"); plt.tight_layout()
plt.show()











import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# ==== Adjust these to your business context ====
BENEFIT_TP = 200.0   # net revenue (LTV) saved when a true churner is correctly targeted and retained
COST_INTERVENTION = 20.0   # per-customer cost of contacting/offer (applies to all predicted positives)
COST_FN = 300.0      # cost of missing a churner (lost revenue / churn loss)
BENEFIT_TN = 0.0     # usually zero; set >0 only if there's explicit value in ignoring safe customers

# y_proba_xgb should already exist from your XGBoost step
probs = y_proba_xgb
y_true = y_test.values

def profit_from_cm(cm):
    tn, fp, fn, tp = cm.ravel()
    # Profit model:
    # + BENEFIT_TP for each TP (saved churners)
    # - COST_INTERVENTION for each contacted customer = (TP + FP)
    # - COST_FN for each missed churner
    # + BENEFIT_TN for each correctly ignored non-churn (usually 0)
    return (tp * BENEFIT_TP) - ((tp + fp) * COST_INTERVENTION) - (fn * COST_FN) + (tn * BENEFIT_TN)

thresholds = np.linspace(0, 1, 501)
profits = []
cms = []

for t in thresholds:
    y_pred = (probs >= t).astype(int)
    cm = confusion_matrix(y_true, y_pred)
    cms.append(cm)
    profits.append(profit_from_cm(cm))

profits = np.array(profits)
best_idx = int(np.argmax(profits))
best_thr_profit = float(thresholds[best_idx])
best_cm = cms[best_idx]
best_profit = float(profits[best_idx])

print(f"Best PROFIT threshold: {best_thr_profit:.3f}")
print(f"Estimated total PROFIT at best threshold: {best_profit:,.0f}")
print("Confusion matrix at best-profit threshold [TN FP; FN TP]:\n", best_cm)

# Plot: Profit vs Threshold
plt.figure(figsize=(7,4))
plt.plot(thresholds, profits, lw=2)
plt.axvline(best_thr_profit, ls='--', label=f'Best profit thr = {best_thr_profit:.2f}')
plt.title("Profit vs Threshold (Cost-/Benefit-aware)")
plt.xlabel("Threshold")
plt.ylabel("Estimated Profit (arbitrary units)")
plt.legend()
plt.tight_layout()
plt.show()





import numpy as np
import pandas as pd
from sklearn.metrics import (
    precision_recall_curve, precision_score, recall_score, f1_score,
    accuracy_score, roc_auc_score, confusion_matrix
)

# --- Use the latest probabilities and y_test from your XGBoost pipeline ---
probs = y_proba_xgb
y_true = y_test.values

# === Costs/benefits (keep consistent with Section 14A) ===
BENEFIT_TP = 200.0
COST_INTERVENTION = 20.0
COST_FN = 300.0
BENEFIT_TN = 0.0

def profit_from_cm(cm):
    tn, fp, fn, tp = cm.ravel()
    return (tp * BENEFIT_TP) - ((tp + fp) * COST_INTERVENTION) - (fn * COST_FN) + (tn * BENEFIT_TN)

# --- Recompute Best-F1 and High-Recall thresholds from the PR curve ---
precisions, recalls, thresholds = precision_recall_curve(y_true, probs)
thr_grid = np.r_[0, thresholds]  # align lengths with precisions/recalls

f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)
thr_best_f1 = float(thr_grid[np.nanargmax(f1s)])

target_recall = 0.80
idx_recall = np.where(recalls >= target_recall)[0]
thr_high_recall = float(thr_grid[idx_recall[-1]]) if len(idx_recall) > 0 else 0.5

# --- Use the profit-optimal threshold computed in 14A ---
thr_profit = best_thr_profit  # variable defined in Section 14A cell

thresholds_to_check = {
    "Best-F1": thr_best_f1,
    "High-Recall (≈0.80)": thr_high_recall,
    "Profit-Optimal": thr_profit
}

rows = []
for name, t in thresholds_to_check.items():
    y_pred = (probs >= t).astype(int)
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec = recall_score(y_true, y_pred)
    f1  = f1_score(y_true, y_pred)
    auc = roc_auc_score(y_true, probs)  # threshold-independent
    cm  = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    prof = profit_from_cm(cm)
    rows.append({
        "Operating Point": name,
        "Threshold": round(t, 3),
        "Accuracy": round(acc, 3),
        "Precision": round(prec, 3),
        "Recall": round(rec, 3),
        "F1": round(f1, 3),
        "ROC-AUC": round(auc, 3),
        "TN": tn, "FP": fp, "FN": fn, "TP": tp,
        "Contacts (TP+FP)": tp + fp,
        "Profit": int(prof)
    })

cmp_df = pd.DataFrame(rows).sort_values("Operating Point")
cmp_df.style.format({"Profit": "{:,}"})





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# === Use the same costs as Section 14A (keep consistent) ===
BENEFIT_TP = 200.0
COST_INTERVENTION = 20.0
COST_FN = 300.0

# === Capacity/Budget: how many customers can you contact? (change this) ===
CONTACT_BUDGET = 400   # <-- adjust to your realistic capacity

# Prepare a ranking by churn probability (descending)
rank_df = pd.DataFrame({
    "proba": y_proba_xgb,
    "y_true": y_test.values
}).sort_values("proba", ascending=False).reset_index(drop=True)

n = len(rank_df)
P = int(rank_df["y_true"].sum())   # total positives (churners)
N = n - P                          # total negatives (non-churners)

# Cumulative TP/FP as we contact top-k customers
cum_tp = rank_df["y_true"].cumsum().values
cum_fp = np.arange(1, n+1) - cum_tp  # contacted count - TP

# For each k contacts:
# Profit(k) = TP_k*BENEFIT_TP - k*COST_INTERVENTION - (P - TP_k)*COST_FN
k = np.arange(1, n+1)
profit_k = (cum_tp * BENEFIT_TP) - (k * COST_INTERVENTION) - ((P - cum_tp) * COST_FN)

# Other cumulative metrics
recall_k = cum_tp / P
precision_k = np.divide(cum_tp, k, out=np.zeros_like(cum_tp, dtype=float), where=k>0)

# Find best k by profit (unconstrained)
best_idx = int(np.argmax(profit_k))
best_k = int(k[best_idx])
best_profit = float(profit_k[best_idx])

# Profit at your capacity
cap_k = int(min(CONTACT_BUDGET, n))
cap_profit = float(profit_k[cap_k-1])
cap_recall = float(recall_k[cap_k-1])
cap_precision = float(precision_k[cap_k-1])

print(f"Total customers: {n} | Total churners (P): {P}")
print(f"Best k by profit: {best_k}  -> Profit = {best_profit:,.0f}, Recall = {recall_k[best_idx]:.3f}, Precision = {precision_k[best_idx]:.3f}")
print(f"Budget k = {cap_k}          -> Profit = {cap_profit:,.0f}, Recall = {cap_recall:.3f}, Precision = {cap_precision:.3f}")

# ---- Plot 1: Cumulative Profit vs N ----
plt.figure(figsize=(7,4))
plt.plot(k, profit_k, label="Cumulative Profit")
plt.axvline(best_k, ls="--", label=f"Best k = {best_k}")
plt.axvline(cap_k, ls=":", label=f"Budget k = {cap_k}")
plt.xlabel("Number of customers contacted (N)")
plt.ylabel("Profit (arbitrary units)")
plt.title("Cumulative Profit vs Number Contacted")
plt.legend()
plt.tight_layout()
plt.show()

# ---- Plot 2: Cumulative Gain & Lift ----
# Baseline expected positives if picking customers at random:
baseline_rate = P / n
expected_tp_random = k * baseline_rate
gain = cum_tp / P  # fraction of all positives captured at top-k

plt.figure(figsize=(7,4))
plt.plot(k, gain, label="Cumulative Gain (model)")
plt.plot(k, expected_tp_random / P, label="Baseline (random)", linestyle="--")
plt.xlabel("Number of customers contacted (N)")
plt.ylabel("Fraction of positives captured")
plt.title("Cumulative Gain Curve")
plt.legend()
plt.tight_layout()
plt.show()

# Lift at selected points (k% of population)
for frac in [0.05, 0.10, 0.20]:
    kk = int(max(1, round(frac * n)))
    lift = (cum_tp[kk-1] / kk) / baseline_rate
    print(f"Lift at top {int(frac*100)}%: {lift:.2f}x  (Precision={precision_k[kk-1]:.3f}, Recall={recall_k[kk-1]:.3f})")





import numpy as np
import pandas as pd

# Reuse rank_df, cum_tp, k, profit_k, etc. from 14C
# If not present (e.g., new session), re-create rank_df quickly:
try:
    rank_df
except NameError:
    rank_df = pd.DataFrame({"proba": y_proba_xgb, "y_true": y_test.values}) \
                .sort_values("proba", ascending=False).reset_index(drop=True)
    n = len(rank_df)
    P = int(rank_df["y_true"].sum())
    k = np.arange(1, n+1)
    cum_tp = rank_df["y_true"].cumsum().values
    cum_fp = k - cum_tp
    # same cost model as before:
    BENEFIT_TP = 200.0
    COST_INTERVENTION = 20.0
    COST_FN = 300.0
    profit_k = (cum_tp * BENEFIT_TP) - (k * COST_INTERVENTION) - ((P - cum_tp) * COST_FN)

# ----- Set your contact budget here -----
CONTACT_BUDGET = 400  # change and re-run to test scenarios

# Profit-optimal k under budget (1..CONTACT_BUDGET)
cap_k = min(CONTACT_BUDGET, len(k))
cap_slice = slice(0, cap_k)  # python is 0-indexed
best_idx_cap = int(np.argmax(profit_k[cap_slice]))
best_k_cap = int(k[best_idx_cap])
best_profit_cap = float(profit_k[best_idx_cap])

# Unconstrained best (already computed earlier)
best_idx_global = int(np.argmax(profit_k))
best_k_global = int(k[best_idx_global])
best_profit_global = float(profit_k[best_idx_global])

# Metrics at those points
P = int(rank_df["y_true"].sum())
recall_cap = float(cum_tp[best_idx_cap] / P)
precision_cap = float(cum_tp[best_idx_cap] / best_k_cap)

recall_global = float(cum_tp[best_idx_global] / P)
precision_global = float(cum_tp[best_idx_global] / best_k_global)

print(f"[Budget ≤ {CONTACT_BUDGET}] Best k = {best_k_cap} | Profit = {best_profit_cap:,.0f} | "
      f"Recall = {recall_cap:.3f} | Precision = {precision_cap:.3f}")

print(f"[Global best]            k = {best_k_global} | Profit = {best_profit_global:,.0f} | "
      f"Recall = {recall_global:.3f} | Precision = {precision_global:.3f}")





import numpy as np
import pandas as pd

# --- If rank_df etc. don’t exist (e.g., fresh kernel), rebuild quickly from your XGBoost outputs ---
try:
    rank_df
    profit_k
    cum_tp
    k
    P
except NameError:
    rank_df = pd.DataFrame({"proba": y_proba_xgb, "y_true": y_test.values}) \
                .sort_values("proba", ascending=False).reset_index(drop=True)
    n = len(rank_df)
    P = int(rank_df["y_true"].sum())
    k = np.arange(1, n+1)
    cum_tp = rank_df["y_true"].cumsum().values
    cum_fp = k - cum_tp

    # --- SAME cost model as before (adjust here if you want to test other scenarios) ---
    BENEFIT_TP = 200.0
    COST_INTERVENTION = 20.0
    COST_FN = 300.0

    profit_k = (cum_tp * BENEFIT_TP) - (k * COST_INTERVENTION) - ((P - cum_tp) * COST_FN)

# === 1) Minimum contact volume (k*) that yields profit >= 0 ===
nonneg = np.where(profit_k >= 0)[0]
if len(nonneg) == 0:
    print("No contact volume yields non-negative profit under current costs/benefits.")
    k_star = None
else:
    k_star = int(k[nonneg[0]])
    print(f"Break-even contact volume k*: {k_star}  (first k with profit ≥ 0)")
    print(f"Profit at k*: {profit_k[nonneg[0]]:,.0f}")

# === 2) Break-even for a fixed budget ===
CONTACT_BUDGET = 400   # <-- adjust and re-run to test other budgets

cap_k = min(CONTACT_BUDGET, len(k))
TPk = int(cum_tp[cap_k-1])
FNk = P - TPk

# Current costs (same as used in profit_k above)
BENEFIT_TP = 200.0
COST_INTERVENTION = 20.0
COST_FN = 300.0

# Current profit at this budget (sanity)
profit_at_budget = (TPk * BENEFIT_TP) - (cap_k * COST_INTERVENTION) - (FNk * COST_FN)
print(f"\n[Budget = {cap_k}] Current profit: {profit_at_budget:,.0f}  |  TP={TPk}, FN={FNk}")

# --- 2a) Minimum TP benefit needed for profit >= 0 at this budget ---
# Profit = TPk*BTP - cap_k*Cint - FNk*CFN >= 0  =>  BTP >= (cap_k*Cint + FNk*CFN) / TPk
if TPk > 0:
    btp_min = (cap_k * COST_INTERVENTION + FNk * COST_FN) / TPk
    print(f"Break-even TP benefit at budget {cap_k}:  BTP ≥ {btp_min:,.2f}")
else:
    print("TP at this budget is 0; cannot compute a meaningful BTP break-even.")

# --- 2b) Maximum per-contact cost to keep profit >= 0 at this budget ---
# Profit >= 0  =>  Cint <= (TPk*BTP - FNk*CFN) / cap_k
cint_max = (TPk * BENEFIT_TP - FNk * COST_FN) / cap_k
print(f"Break-even contact cost at budget {cap_k}:  Cint ≤ {cint_max:,.2f}")

# Optional: summarize nicely
summary = pd.DataFrame({
    "Metric": ["Break-even k*", "Profit at k*", 
               f"Profit at budget={cap_k}", 
               "Min TP benefit (BTP) for budget", 
               "Max contact cost (Cint) for budget"],
    "Value": [k_star, 
              None if k_star is None else f"{profit_k[k_star-1]:,.0f}",
              f"{profit_at_budget:,.0f}",
              None if TPk==0 else f"{btp_min:,.2f}",
              f"{cint_max:,.2f}"]
})
print("\nSummary:")
display(summary)








# 15A. Probability Calibration — Isotonic on XGBoost (with the same preprocessor)

from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from sklearn.metrics import brier_score_loss, roc_auc_score, average_precision_score
from xgboost import XGBClassifier
from sklearn.pipeline import Pipeline

# Reuse your preprocessor 'preprocessor' and training split X_train, y_train, X_test, y_test
# 1) Recreate the same base XGB as before (match your prior hyperparams)
xgb_base = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=(y_train.value_counts()[0] / y_train.value_counts()[1]),
    random_state=42,
    n_jobs=-1,
    use_label_encoder=False,
    eval_metric='logloss'
)

# 2) Pipeline: preprocessor -> XGB -> Calibrator (isotonic)
pipe_xgb = Pipeline(steps=[("prep", preprocessor), ("xgb", xgb_base)])
calibrated = CalibratedClassifierCV(pipe_xgb, cv=5, method="isotonic")  # cross-validated calibration

# 3) Fit & predict
calibrated.fit(X_train, y_train)
probs_cal = calibrated.predict_proba(X_test)[:, 1]

# 4) Evaluate vs your existing (uncalibrated) probabilities y_proba_xgb
from sklearn.metrics import precision_recall_curve
import matplotlib.pyplot as plt
import numpy as np

# If you don't still have y_proba_xgb in memory, compute quickly:
try:
    y_proba_xgb
except NameError:
    y_proba_xgb = xgb_clf.predict_proba(X_test)[:, 1]

# Core metrics
brier_uncal = brier_score_loss(y_test, y_proba_xgb)
brier_cal   = brier_score_loss(y_test, probs_cal)
roc_uncal   = roc_auc_score(y_test, y_proba_xgb)
roc_cal     = roc_auc_score(y_test, probs_cal)
ap_uncal    = average_precision_score(y_test, y_proba_xgb)  # PR AUC
ap_cal      = average_precision_score(y_test, probs_cal)

print("=== Calibration / Discrimination Metrics ===")
print(f"Brier score (lower=better):  Uncal={brier_uncal:.4f}  |  Calibrated={brier_cal:.4f}")
print(f"ROC-AUC:                     Uncal={roc_uncal:.3f}   |  Calibrated={roc_cal:.3f}")
print(f"PR-AUC (Average Precision):  Uncal={ap_uncal:.3f}    |  Calibrated={ap_cal:.3f}")

# 5) Reliability curve (calibration curve)
prob_true_uncal, prob_pred_uncal = calibration_curve(y_test, y_proba_xgb, n_bins=10, strategy='quantile')
prob_true_cal,   prob_pred_cal   = calibration_curve(y_test, probs_cal,   n_bins=10, strategy='quantile')

plt.figure(figsize=(5.5,5))
plt.plot([0,1],[0,1], 'k--', label="Perfectly calibrated")
plt.plot(prob_pred_uncal, prob_true_uncal, marker='o', label="Uncalibrated XGB")
plt.plot(prob_pred_cal, prob_true_cal, marker='o', label="Calibrated (Isotonic)")
plt.xlabel("Predicted probability")
plt.ylabel("Observed fraction positive")
plt.title("Reliability Curve (Calibration)")
plt.legend()
plt.tight_layout()
plt.show()

# 6) (Optional preview) quick threshold at 0.5 compare confusion matrices
from sklearn.metrics import confusion_matrix
cm_uncal = confusion_matrix(y_test, (y_proba_xgb >= 0.5).astype(int))
cm_cal   = confusion_matrix(y_test, (probs_cal   >= 0.5).astype(int))
print("\nConfusion @ 0.5 threshold (Uncalibrated):\n", cm_uncal)
print("Confusion @ 0.5 threshold (Calibrated):\n", cm_cal)

# Save calibrated probabilities for later threshold/profit steps if you want to swap them in:
y_proba_xgb_calibrated = probs_cal





import numpy as np
import pandas as pd
from sklearn.metrics import (
    precision_recall_curve, precision_score, recall_score, f1_score,
    accuracy_score, roc_auc_score, confusion_matrix
)

# Use calibrated probabilities if available; else fall back to uncalibrated
probs = globals().get("y_proba_xgb_calibrated", None)
if probs is None:
    print("Warning: calibrated probs not found; using uncalibrated y_proba_xgb.")
    probs = y_proba_xgb

y_true = y_test.values

# --- Same cost model as Section 14A (keep consistent) ---
BENEFIT_TP = 200.0
COST_INTERVENTION = 20.0
COST_FN = 300.0
BENEFIT_TN = 0.0

def profit_from_cm(cm):
    tn, fp, fn, tp = cm.ravel()
    return (tp * BENEFIT_TP) - ((tp + fp) * COST_INTERVENTION) - (fn * COST_FN) + (tn * BENEFIT_TN)

# ---- Find best-F1 and high-recall thresholds from PR curve (on *calibrated* probs) ----
precisions, recalls, thresholds = precision_recall_curve(y_true, probs)
thr_grid = np.r_[0, thresholds]    # align with precisions/recalls sizes

f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-9)
thr_best_f1 = float(thr_grid[np.nanargmax(f1s)])

target_recall = 0.80
idx_recall = np.where(recalls >= target_recall)[0]
thr_high_recall = float(thr_grid[idx_recall[-1]]) if len(idx_recall) > 0 else 0.5

# ---- Profit-optimal threshold (grid search over PR thresholds) ----
profits = []
for t in thr_grid:
    y_pred = (probs >= t).astype(int)
    cm = confusion_matrix(y_true, y_pred)
    profits.append(profit_from_cm(cm))
best_idx_profit = int(np.argmax(profits))
thr_profit = float(thr_grid[best_idx_profit])

# ---- Evaluate the three operating points ----
def evaluate(name, t):
    y_pred = (probs >= t).astype(int)
    cm = confusion_matrix(y_true, y_pred)
    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred)
    rec  = recall_score(y_true, y_pred)
    f1   = f1_score(y_true, y_pred)
    auc  = roc_auc_score(y_true, probs)  # same across rows
    tn, fp, fn, tp = cm.ravel()
    prof = profit_from_cm(cm)
    return {
        "Operating Point": name,
        "Threshold": round(t, 3),
        "Accuracy": round(acc, 3),
        "Precision": round(prec, 3),
        "Recall": round(rec, 3),
        "F1": round(f1, 3),
        "ROC-AUC": round(auc, 3),
        "TN": tn, "FP": fp, "FN": fn, "TP": tp,
        "Contacts (TP+FP)": tp + fp,
        "Profit": int(prof)
    }

rows = [
    evaluate("Best-F1 (calibrated)", thr_best_f1),
    evaluate("High-Recall≈0.80 (calibrated)", thr_high_recall),
    evaluate("Profit-Optimal (calibrated)", thr_profit),
]

cmp_cal = pd.DataFrame(rows).sort_values("Operating Point")
cmp_cal.style.format({"Profit": "{:,}"})





# Pick one and freeze it for downstream use (API/Streamlit, etc.)
FINAL_THRESHOLD = 0.062   # Profit-Optimal (calibrated)
# FINAL_THRESHOLD = 0.279 # Best-F1 (calibrated)

print("Using FINAL_THRESHOLD =", FINAL_THRESHOLD)





# 16. Export Calibrated Model & Metadata (for Deployment)

import os, json, joblib, datetime as dt
import numpy as np

# --- 1) Ensure we have the calibrated model & chosen threshold ---
assert 'calibrated' in globals(), "Calibrated model object not found. Run Section 15A first."
assert 'FINAL_THRESHOLD' in globals(), "Set FINAL_THRESHOLD first (see previous cell)."

# --- 2) Create output folder ---
MODEL_DIR = "../outputs/models"
os.makedirs(MODEL_DIR, exist_ok=True)

# --- 3) Build artifact names (timestamped) ---
ts = dt.datetime.now().strftime("%Y%m%d_%H%M%S")
model_path = os.path.join(MODEL_DIR, f"telco_xgb_isotonic_{ts}.joblib")
meta_path  = os.path.join(MODEL_DIR, f"telco_xgb_isotonic_{ts}.meta.json")

# --- 4) Collect metadata for reliable reuse ---
# Try to recover feature names after preprocessing (useful for debugging)
try:
    prep = calibrated.base_estimator.named_steps["prep"]
    ohe  = prep.named_transformers_["cat"]
    cat_feature_names = ohe.get_feature_names_out(categorical_cols).tolist()
    feature_names_tx = list(numerical_cols) + cat_feature_names
except Exception:
    feature_names_tx = None  # not critical

metadata = {
    "created_at": ts,
    "model_type": "XGBClassifier + IsotonicCalibration (wrapped in Pipeline)",
    "calibration": "isotonic",
    "threshold": float(FINAL_THRESHOLD),
    "train_rows": int(X_train.shape[0]),
    "test_rows": int(X_test.shape[0]),
    "numerical_cols": numerical_cols,
    "categorical_cols": categorical_cols,
    "transformed_feature_names": feature_names_tx,
    "costs": {
        "BENEFIT_TP": 200.0,
        "COST_INTERVENTION": 20.0,
        "COST_FN": 300.0,
        "BENEFIT_TN": 0.0
    }
}

# --- 5) Save model + metadata ---
joblib.dump(calibrated, model_path)
with open(meta_path, "w") as f:
    json.dump(metadata, f, indent=2)

print("Saved:")
print(" • Model   ->", model_path)
print(" • Metadata->", meta_path)

# --- 6) Quick integrity check: load and compare a few predictions ---
loaded_model = joblib.load(model_path)
probs_loaded = loaded_model.predict_proba(X_test)[:, 1]
delta = float(np.max(np.abs(probs_loaded - y_proba_xgb_calibrated)))
print(f"Max abs diff vs in-memory calibrated probs: {delta:.6f} (should be ~0)")

# --- 7) Convenience helper for future inference (batch or single row) ---
def predict_churn_proba(df_batch, model=loaded_model):
    """Returns predicted churn probabilities for a raw batch DataFrame."""
    return model.predict_proba(df_batch)[:, 1]

def predict_churn_label(df_batch, threshold=FINAL_THRESHOLD, model=loaded_model):
    """Returns 0/1 churn labels using your chosen FINAL_THRESHOLD."""
    return (predict_churn_proba(df_batch, model) >= threshold).astype(int)

print("Helper functions ready: predict_churn_proba(df), predict_churn_label(df, threshold)")





# 17. Inference — Batch Scoring & Single-Customer Example

import pandas as pd
import numpy as np
import os

# 1) Score a batch (use your held-out test set as a demo)
probs = predict_churn_proba(X_test)                 # calibrated probabilities
labels = predict_churn_label(X_test, FINAL_THRESHOLD)

scored = X_test.copy().reset_index(drop=True)
scored["churn_proba"] = probs
scored["churn_label"] = labels

# Show Top-10 highest risk customers (by probability)
top10 = scored.sort_values("churn_proba", ascending=False).head(10)
display(top10[["churn_proba", "churn_label"] + [c for c in scored.columns if c not in ["churn_proba","churn_label"]]][:10])

# 2) Save scored test set (handy for reports / Streamlit table)
out_csv = "../outputs/scored/scored_test_calibrated.csv"
os.makedirs(os.path.dirname(out_csv), exist_ok=True)
scored.to_csv(out_csv, index=False)
print("Scored test set saved to:", out_csv)

# 3) Single-customer inference (raw dict -> DataFrame -> proba/label)
#    Example uses plausible values from the Telco schema. Adjust as needed.
new_customer = {
    "gender": "Female",
    "SeniorCitizen": 0,
    "Partner": "No",
    "Dependents": "No",
    "tenure": 5,
    "PhoneService": "Yes",
    "MultipleLines": "No",
    "InternetService": "Fiber optic",
    "OnlineSecurity": "No",
    "OnlineBackup": "No",
    "DeviceProtection": "No",
    "TechSupport": "No",
    "StreamingTV": "Yes",
    "StreamingMovies": "Yes",
    "Contract": "Month-to-month",
    "PaperlessBilling": "Yes",
    "PaymentMethod": "Electronic check",
    "MonthlyCharges": 90.5,
    "TotalCharges": 452.0,
}

new_df = pd.DataFrame([new_customer])
new_proba = predict_churn_proba(new_df)[0]
new_label = predict_churn_label(new_df, FINAL_THRESHOLD)[0]

print(f"\nSingle-customer prediction -> churn_proba={new_proba:.3f}, label={int(new_label)} (threshold={FINAL_THRESHOLD})")





# 18. Slice-Based Error Analysis (Who We Miss / Over-Flag)

import pandas as pd
import numpy as np
from sklearn.metrics import precision_score, recall_score, f1_score

# Use calibrated probabilities from Section 15 and FINAL_THRESHOLD from 15C
assert 'y_proba_xgb_calibrated' in globals(), "Run calibration section first."
assert 'FINAL_THRESHOLD' in globals(), "Set FINAL_THRESHOLD first."

# Build a test dataframe with y_true and y_pred at the chosen threshold
df_test_view = X_test.copy().reset_index(drop=True)
df_test_view["y_true"] = y_test.values
df_test_view["proba"]  = y_proba_xgb_calibrated
df_test_view["y_pred"] = (df_test_view["proba"] >= FINAL_THRESHOLD).astype(int)

# Define a few business-relevant slices (feel free to add more)
slices = {
    "Contract": ["Month-to-month", "One year", "Two year"],
    "InternetService": ["DSL", "Fiber optic", "No"],
    "SeniorCitizen": [0, 1],
    "PaymentMethod": ["Electronic check", "Mailed check",
                      "Bank transfer (automatic)", "Credit card (automatic)"]
}

rows = []
for col, values in slices.items():
    for v in values:
        mask = (df_test_view[col] == v) if df_test_view[col].dtype == object else (df_test_view[col] == v)
        if mask.sum() == 0:
            continue
        yt = df_test_view.loc[mask, "y_true"].values
        yp = df_test_view.loc[mask, "y_pred"].values
        prec = precision_score(yt, yp, zero_division=0)
        rec  = recall_score(yt, yp, zero_division=0)
        f1   = f1_score(yt, yp, zero_division=0)
        rows.append([col, v, mask.sum(), prec, rec, f1,
                     int(((yt==1)&(yp==0)).sum()),   # FN
                     int(((yt==0)&(yp==1)).sum())])  # FP

slice_df = pd.DataFrame(rows, columns=[
    "Slice Feature", "Slice Value", "N", "Precision", "Recall", "F1", "False Negatives (missed churn)", "False Positives (over-flag)"
]).sort_values(["Slice Feature","Slice Value"])

# Display summary
slice_df.style.format({"Precision":"{:.3f}","Recall":"{:.3f}","F1":"{:.3f}"})





# 18B. Retention Playbook — Actions by Segment (from Slice Metrics)

play = slice_df.copy()

# Rates
play["FP_rate"] = (play["False Positives (over-flag)"] / play["N"]).round(3)
play["FN_rate"] = (play["False Negatives (missed churn)"] / play["N"]).round(3)

# Simple, transparent rules -> strategy suggestions
def recommend(row):
    prec, rec, fp_r, fn_r, n = row["Precision"], row["Recall"], row["FP_rate"], row["FN_rate"], row["N"]

    # High-risk, high-volume segments (catch almost all churners but many FPs)
    if rec >= 0.95 and prec < 0.45 and n >= 300:
        return "Automated outreach + small incentive; consider raising cohort sub-threshold for cost control"

    # Missed-churn concern
    if rec < 0.85 or fn_r > 0.08:
        return "Lower cohort threshold by ~0.03; prioritize human follow-ups for top scores"

    # Reasonable balance
    if prec >= 0.45 and rec >= 0.90:
        return "Keep threshold; mixed outreach (email + selective calls)"

    # Low base risk (contracts ≥ 1 year) with low precision
    if row["Slice Feature"] == "Contract" and row["Slice Value"] in ["One year", "Two year"]:
        return "Tighten cohort threshold; contact only top 5–10% by score"

    # Default
    return "Automated outreach; monitor conversion & adjust threshold ±0.02"

play["Recommendation"] = play.apply(recommend, axis=1)

# Order for readability
order_cols = [
    "Slice Feature", "Slice Value", "N", "Precision", "Recall", "F1",
    "FP_rate", "FN_rate", "False Positives (over-flag)", "False Negatives (missed churn)",
    "Recommendation"
]
playbook = play[order_cols].sort_values(["Slice Feature", "Slice Value"]).reset_index(drop=True)

# Display
playbook.style.format({
    "Precision":"{:.3f}","Recall":"{:.3f}","F1":"{:.3f}",
    "FP_rate":"{:.3f}","FN_rate":"{:.3f}"
})





# Fairness / Responsible-AI mini-audit on key demographics
import numpy as np
import pandas as pd
from sklearn.metrics import precision_score, recall_score, f1_score

# --- Inputs we rely on from earlier steps ---
# X_test: original (untransformed) test features as DataFrame
# y_test: ground-truth labels (Series)
# y_proba_xgb_calibrated: calibrated churn probabilities on X_test (numpy array)
# FINAL_THRESHOLD: your chosen calibrated decision threshold (float)

def metrics_for_slice(mask, probs, y_true, thr):
    y_pred = (probs[mask] >= thr).astype(int)
    return {
        "N": int(mask.sum()),
        "Precision": precision_score(y_true[mask], y_pred, zero_division=0),
        "Recall":    recall_score(y_true[mask],  y_pred, zero_division=0),
        "F1":        f1_score(y_true[mask],      y_pred, zero_division=0),
    }

rows = []

# 1) Gender slices
for g in ["Female", "Male"]:
    m = (X_test["gender"] == g).values
    res = metrics_for_slice(m, y_proba_xgb_calibrated, y_test.values, FINAL_THRESHOLD)
    rows.append({"Slice Feature":"gender", "Slice Value":g, **res})

# 2) SeniorCitizen slices (0/1)
for s in [0, 1]:
    m = (X_test["SeniorCitizen"] == s).values
    res = metrics_for_slice(m, y_proba_xgb_calibrated, y_test.values, FINAL_THRESHOLD)
    rows.append({"Slice Feature":"SeniorCitizen", "Slice Value":s, **res})

fair_df = pd.DataFrame(rows)
display(fair_df.style.format({"Precision":"{:.3f}", "Recall":"{:.3f}", "F1":"{:.3f}"}))

# Simple gap summary to call out disparities
def gap(a, b): 
    return round(a - b, 3)

summary = []
# Gender gap (Recall & Precision)
g_f = fair_df[(fair_df["Slice Feature"]=="gender") & (fair_df["Slice Value"]=="Female")].iloc[0]
g_m = fair_df[(fair_df["Slice Feature"]=="gender") & (fair_df["Slice Value"]=="Male")].iloc[0]
summary.append({"Gap Metric":"Gender Recall gap (Female - Male)", "Gap": gap(g_f["Recall"], g_m["Recall"])})
summary.append({"Gap Metric":"Gender Precision gap (Female - Male)", "Gap": gap(g_f["Precision"], g_m["Precision"])})

# SeniorCitizen gap
s0 = fair_df[(fair_df["Slice Feature"]=="SeniorCitizen") & (fair_df["Slice Value"]==0)].iloc[0]
s1 = fair_df[(fair_df["Slice Feature"]=="SeniorCitizen") & (fair_df["Slice Value"]==1)].iloc[0]
summary.append({"Gap Metric":"SeniorCitizen Recall gap (1 - 0)", "Gap": gap(s1["Recall"], s0["Recall"])})
summary.append({"Gap Metric":"SeniorCitizen Precision gap (1 - 0)", "Gap": gap(s1["Precision"], s0["Precision"])})

gap_df = pd.DataFrame(summary)
display(gap_df)








from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.pipeline import Pipeline
from xgboost import XGBClassifier
import numpy as np
import pandas as pd

# Assumes you already have: X, y, and `preprocessor` (ColumnTransformer)

# Compute class imbalance ratio once (negatives / positives)
spw = (y.value_counts()[0] / y.value_counts()[1])

xgb_base = XGBClassifier(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    scale_pos_weight=spw,   # handle imbalance
    eval_metric='logloss',  # explicit metric (keeps XGB quiet)
    tree_method='hist',     # faster on CPU
    random_state=42,
    n_jobs=-1,
    verbosity=0             # silence training logs (optional)
)

pipe_cv = Pipeline(steps=[("prep", preprocessor), ("model", xgb_base)])

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scoring = {"ROC-AUC": "roc_auc", "PR-AUC": "average_precision"}

res = cross_validate(
    pipe_cv, X, y,
    scoring=scoring,
    cv=cv,
    n_jobs=-1,
    return_train_score=False
)

def mean_std(a):
    return f"{np.mean(a):.3f} ± {np.std(a):.3f}"

summary = pd.DataFrame({
    "Metric": ["ROC-AUC", "PR-AUC"],
    "Mean ± Std": [mean_std(res['test_ROC-AUC']), mean_std(res['test_PR-AUC'])]
})

summary





# Reproducibility: set seeds and record package versions
import os, random, json, sys, platform, numpy as np
import sklearn, xgboost, shap, pandas as pd

SEED = 42
random.seed(SEED); np.random.seed(SEED); os.environ["PYTHONHASHSEED"] = str(SEED)

env_summary = {
    "python": sys.version.split()[0],
    "platform": platform.platform(),
    "packages": {
        "pandas": pd.__version__,
        "numpy": np.__version__,
        "scikit_learn": sklearn.__version__,
        "xgboost": xgboost.__version__,
        "shap": shap.__version__
    },
    "seed": SEED
}
print(json.dumps(env_summary, indent=2))


from pathlib import Path
import json

Path("../outputs/meta").mkdir(parents=True, exist_ok=True)
with open("../outputs/meta/env_summary.json", "w") as f:
    json.dump(env_summary, f, indent=2)

print("Saved env to ../outputs/meta/env_summary.json")








# === TL;DR — Final Model & Decision Policy ===
import os, json, time
import numpy as np
import pandas as pd
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, average_precision_score, brier_score_loss,
    confusion_matrix
)

# ---- 1) Pick probabilities & threshold robustly ----
# Prefer calibrated probs if available, else fall back.
probs = None
for cand in ["y_proba_xgb_calibrated", "y_proba_xgb"]:
    if cand in globals():
        probs = globals()[cand]
        break
if probs is None:
    raise RuntimeError("No probability array found. Define y_proba_xgb_calibrated or y_proba_xgb first.")

# Prefer finalized threshold if available, else fall back to 0.5
if "FINAL_THRESHOLD" in globals():
    threshold = float(FINAL_THRESHOLD)
elif "best_thr_profit" in globals():
    threshold = float(best_thr_profit)
else:
    threshold = 0.5  # safe fallback

# ---- 2) Default business costs (use your earlier values if set) ----
BENEFIT_TP       = globals().get("BENEFIT_TP", 200.0)
COST_INTERVENTION= globals().get("COST_INTERVENTION", 20.0)
COST_FN          = globals().get("COST_FN", 300.0)
BENEFIT_TN       = globals().get("BENEFIT_TN", 0.0)

def profit_from_cm(cm):
    tn, fp, fn, tp = cm.ravel()
    return (tp * BENEFIT_TP) - ((tp + fp) * COST_INTERVENTION) - (fn * COST_FN) + (tn * BENEFIT_TN)

# ---- 3) Metrics @ final operating point ----
y_true = y_test.values
y_pred = (probs >= threshold).astype(int)
cm = confusion_matrix(y_true, y_pred)
tn, fp, fn, tp = cm.ravel()

acc  = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred, zero_division=0)
rec  = recall_score(y_true, y_pred, zero_division=0)
f1   = f1_score(y_true, y_pred, zero_division=0)
auc  = roc_auc_score(y_true, probs)
prauc= average_precision_score(y_true, probs)
brier= brier_score_loss(y_true, probs)
contacts = tp + fp
profit = profit_from_cm(cm)

# ---- 4) Pretty print ----
print("=== TL;DR — Final Decision Policy ===")
print(f"Operating threshold: {threshold:.3f}")
print(f"Business costs: TP+{BENEFIT_TP:.0f}, Intervention-{COST_INTERVENTION:.0f}, FN-{COST_FN:.0f}, TN+{BENEFIT_TN:.0f}")
print("\n--- Key Test Metrics ---")
print(f"ROC-AUC:   {auc:.3f}    | PR-AUC: {prauc:.3f} | Brier: {brier:.4f}")
print(f"Accuracy:  {acc:.3f}    | Precision: {prec:.3f} | Recall: {rec:.3f} | F1: {f1:.3f}")
print("\n--- Confusion Matrix [TN FP; FN TP] ---")
print(cm)
print(f"\nContacts to make (TP+FP): {contacts}")
print(f"Estimated profit: {profit:,.0f}")

# Tabular summary for the notebook
summary_df = pd.DataFrame({
    "Operating Threshold":[threshold],
    "ROC-AUC":[auc],
    "PR-AUC":[prauc],
    "Brier":[brier],
    "Accuracy":[acc],
    "Precision":[prec],
    "Recall":[rec],
    "F1":[f1],
    "TN":[tn], "FP":[fp], "FN":[fn], "TP":[tp],
    "Contacts (TP+FP)":[contacts],
    "Estimated Profit":[profit]
})
display(summary_df.round(3))

# ---- 5) Save compact artifacts ----
stamp = time.strftime("%Y%m%d_%H%M%S")
out_dir = "../outputs/summary"
os.makedirs(out_dir, exist_ok=True)

summary_json = {
    "timestamp": stamp,
    "threshold": threshold,
    "metrics": {
        "roc_auc": float(auc),
        "pr_auc": float(prauc),
        "brier": float(brier),
        "accuracy": float(acc),
        "precision": float(prec),
        "recall": float(rec),
        "f1": float(f1)
    },
    "confusion_matrix": {"tn": int(tn), "fp": int(fp), "fn": int(fn), "tp": int(tp)},
    "contacts": int(contacts),
    "estimated_profit": int(profit),
    "costs": {
        "benefit_tp": BENEFIT_TP,
        "cost_intervention": COST_INTERVENTION,
        "cost_fn": COST_FN,
        "benefit_tn": BENEFIT_TN
    }
}

json_path = os.path.join(out_dir, f"tldr_summary_{stamp}.json")
csv_path  = os.path.join(out_dir, f"tldr_summary_{stamp}.csv")

with open(json_path, "w") as f:
    json.dump(summary_json, f, indent=2)
summary_df.to_csv(csv_path, index=False)

print(f"\nSaved TL;DR summary to:\n • JSON: {json_path}\n • CSV : {csv_path}")



